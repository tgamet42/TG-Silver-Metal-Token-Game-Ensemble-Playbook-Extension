{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":5794968,"sourceType":"datasetVersion","datasetId":3327426},{"sourceId":6865136,"sourceType":"datasetVersion","datasetId":3945154},{"sourceId":6890527,"sourceType":"datasetVersion","datasetId":3942644},{"sourceId":6901341,"sourceType":"datasetVersion","datasetId":3960967},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256}],"dockerImageVersionId":30627,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook is both a learning exercise and contest entry\nAll changes are under the MIT License.\nI am building upon DIAGT related work as the vast majority of other contestants have also leveraged it. I am starting with Wilmer E. Henao's notebook as it claims to have an initial high LB score, is recent, and appears well structured/built.\n\nThe first submission is Henao's baseline. Alterations will follow. I am initially using the T4 x2 accelerator.\nThe baseline run did not actually run. First attempt through all cells showed that catboost is not accepting the features. Instead of trying to fix it, I've removed it and added its weight to lgb. I believe the current code will finish execution and am running a new baseline.\n\n1. The first run failed as the code did not make an actual submission and ended with no real work done.\n2. The second attempt successed with the code block that prevented an actual submission from being computed removed. Also, catboost caused errors and was commented out.\n3. There were three runs with different weights for NB, SGD, and LGB, all three producing an LB of 0.956.\n4. LGB alone produced an LB of 0.898\n5. Seeing if I can get TabPFN to work and what LB score it produces. Unfortunately, TabPFN only runs with 1 single configuration on the test data, but runs out of memory on the public data test run. Versions 6, 7,and 8 were attempts. The val_loss was also coming back with 0.5 suggesting little more than 50/50 odds. I am leaving the code as commented out or marked down.\n6. NB scored 0.926 and SDG scored 0.940 on the LB.\n7. Tried weights [0.34,0.38,0.28] but the LB was only 0.953.\n8. Seeing if DenseNet can improve results. This was a journey trying to get it to run without exceptions or out of memory errors. I learned about sparse matrix and sparse tensors in tensor flow. I also saw odd behavior in training and then realized that we have a leak - it is loading the vocab from the test data which means it is training with words that do not exist in the training essays. However, it still just times out with both the 'SPE' and 'BPE' models so I'm cutting the models in half.\n9. The half set of models (one NB, one SDG, one LGB, and one DenseNet) also using only the training set's vocabulary scored 0.893 with weights 0.07, 0.31, 0.31, and 0.31. Ironically, just the new DenseNet scored 0.921. See v36 and v37 respectively.\n10. Running full experiment, training vocab only, weights NB=0.07, SDG=0.31, LGB=0.31, and DN=0.31. (DN is training with an 80/20 split). This is v38.\n11. Running full experiment, test vocab, weights NB=0.07, SDG=0.31, LGB=0.31, and DN=0.31. (DN is training with an 80/20 split). This is v39.\n12. OK, from an LB perspective, it scores higher when trained with all words in the test set instead of just the words in the training set. I presume it can relate words unique to the test set to words in the training set to help improve accuracy / lower loss. \n13. Added CatBoost as a 5th model, and changed the weights to NB=0.07, SDG=0.24, LGB=0.23, DN=0.23, and CB=0.23. Will build as v40.\n14. OutOfMemory (OOM) is a bother, let's see if we can make the first \"BPE\" pass without it. Also found a curious item, it looks like the y_train was loaded with the 'labels' column, but the 'labels' column was left in the train data (it is now dropped). This produced a LB score of 0.960, the highest score yet. \n15. Someone of a wild stab. Past evidence suggests LGB is the lowest scoring contributor, next come DN and NB, then we have SGD and CatBoost. They are entered NB, SGD, LGB, ND, and CatBoost as [0.10, 0.35, 0.05, 0.10, 0.40].  This produced the highest LB score of 0.960.\n16. Setting up the first attempt to dynamically determine weights by ratio of the roc_auc scores coming back from the individual models based upon ratios of the area above the curve (the negative effects). Retry will be needed, reogranized things so the internal training set predictions are only kept for one model at a time and their roc_auc is the only thing perserved for later use. We ran into OOM on trying to keep 5 extra predictions in memory (it's really close to its limits). The result was bad, it scored 0.946 on the LB. \n17. I am going to also give Nelder-Mead optimization one try with the weights. ","metadata":{}},{"cell_type":"markdown","source":"# Comprehensive Analysis and Modeling Notebook\n\nWelcome to this comprehensive notebook where we dive deep into the world of data analysis and machine learning. This document is meticulously crafted to guide you through various stages of data processing, modeling, and prediction. Here's what to expect:\n\n## What This Notebook Offers:\n1. **Data Preprocessing**: Initial steps to clean and prepare the data for analysis.\n2. **Exploratory Data Analysis (EDA)**: Insights and patterns unraveled through visual and statistical methods.\n3. **Feature Engineering**: Enhancing the dataset with new, informative features.\n4. **Model Development**: Implementation of various machine learning models, including both traditional and advanced techniques.\n5. **Evaluation and Optimization**: Assessing model performance and tuning them for better accuracy.\n6. **Ensemble Techniques**: Leveraging the power of multiple models to improve predictions.\n7. **Final Predictions and Submission**: Preparing the final predictions for submission, demonstrating the practical application of our analysis.\n\n## Intended Audience:\nThis notebook is designed for both beginners and experienced practitioners in the field of data science. Whether you're looking to learn new skills, seeking to understand specific methodologies, or aiming to apply advanced techniques in machine learning, this notebook has something to offer.\n\n## Feedback and Collaboration:\nYour feedback is highly appreciated! If you have any suggestions, questions, or ideas for improvement, please feel free to share. Collaboration is the key to success in the ever-evolving field of data science, and your input is invaluable.\n\n---\n\nLet's embark on this data science journey together and uncover the stories hidden within the data!\n","metadata":{"papermill":{"duration":0.010541,"end_time":"2023-12-26T15:12:33.547876","exception":false,"start_time":"2023-12-26T15:12:33.537335","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import sys\nimport gc\n\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n    SentencePieceBPETokenizer\n)\n\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizerFast\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\n\nfrom tensorflow.keras.metrics import AUC\nfrom sklearn.model_selection import train_test_split","metadata":{"papermill":{"duration":17.838201,"end_time":"2023-12-26T15:12:51.396382","exception":false,"start_time":"2023-12-26T15:12:33.558181","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-15T20:38:11.719218Z","iopub.execute_input":"2024-01-15T20:38:11.719839Z","iopub.status.idle":"2024-01-15T20:38:34.675507Z","shell.execute_reply.started":"2024-01-15T20:38:11.719811Z","shell.execute_reply":"2024-01-15T20:38:34.67454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Concatenate, Dense, Dropout, LSTM, BatchNormalization, Activation, Input\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv1D, Conv2D, Flatten, Input, MaxPooling1D, MaxPooling2D, concatenate\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.base import BaseEstimator, ClassifierMixin","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:38:34.677136Z","iopub.execute_input":"2024-01-15T20:38:34.677857Z","iopub.status.idle":"2024-01-15T20:38:34.687716Z","shell.execute_reply.started":"2024-01-15T20:38:34.677824Z","shell.execute_reply":"2024-01-15T20:38:34.686785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Configure Strategy. Assume TPU...if not set default for GPU\ntpu = None\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=\"local\") # \"local\" for 1VM TPU\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print(\"on TPU\")\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\nexcept:\n    strategy = tf.distribute.get_strategy()","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:38:34.689291Z","iopub.execute_input":"2024-01-15T20:38:34.689608Z","iopub.status.idle":"2024-01-15T20:38:35.408444Z","shell.execute_reply.started":"2024-01-15T20:38:34.689581Z","shell.execute_reply":"2024-01-15T20:38:35.40751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# pip install catboost[gpu]","metadata":{"execution":{"iopub.status.busy":"2024-01-15T19:52:06.784938Z","iopub.execute_input":"2024-01-15T19:52:06.785333Z","iopub.status.idle":"2024-01-15T19:52:40.461023Z","shell.execute_reply.started":"2024-01-15T19:52:06.785297Z","shell.execute_reply":"2024-01-15T19:52:40.45979Z"}}},{"cell_type":"code","source":"INITIAL_SEED = 4221\n\n# For reproducability, lock down the random seeds\nimport random\nimport os\nimport tensorflow as tf\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(INITIAL_SEED) # best try to make runs reproducible - can also tweak this as it affects training splits","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:38:35.410699Z","iopub.execute_input":"2024-01-15T20:38:35.411031Z","iopub.status.idle":"2024-01-15T20:38:35.417453Z","shell.execute_reply.started":"2024-01-15T20:38:35.411002Z","shell.execute_reply":"2024-01-15T20:38:35.416315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nprint(sys.version)","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:38:35.418781Z","iopub.execute_input":"2024-01-15T20:38:35.419138Z","iopub.status.idle":"2024-01-15T20:38:35.42791Z","shell.execute_reply.started":"2024-01-15T20:38:35.4191Z","shell.execute_reply":"2024-01-15T20:38:35.426972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\nsub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\ntrain = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-01-15T20:38:35.43103Z","iopub.execute_input":"2024-01-15T20:38:35.431345Z","iopub.status.idle":"2024-01-15T20:38:37.536053Z","shell.execute_reply.started":"2024-01-15T20:38:35.431322Z","shell.execute_reply":"2024-01-15T20:38:37.535022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Test\")\nprint(test.head(5))\nprint(test.describe())\nprint (\"train\")\nprint(train.head(5))\nprint(train.describe())","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:38:37.537315Z","iopub.execute_input":"2024-01-15T20:38:37.537629Z","iopub.status.idle":"2024-01-15T20:38:37.5704Z","shell.execute_reply.started":"2024-01-15T20:38:37.537603Z","shell.execute_reply":"2024-01-15T20:38:37.569397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop_duplicates(subset=['text'])\ntrain.reset_index(drop=True, inplace=True)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-01-15T20:38:37.571692Z","iopub.execute_input":"2024-01-15T20:38:37.571983Z","iopub.status.idle":"2024-01-15T20:38:37.649929Z","shell.execute_reply.started":"2024-01-15T20:38:37.571957Z","shell.execute_reply":"2024-01-15T20:38:37.648991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOWERCASE = False\nVOCAB_SIZE = 64000","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-01-15T20:38:37.651257Z","iopub.execute_input":"2024-01-15T20:38:37.651661Z","iopub.status.idle":"2024-01-15T20:38:37.659381Z","shell.execute_reply.started":"2024-01-15T20:38:37.651625Z","shell.execute_reply":"2024-01-15T20:38:37.658558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Byte-Pair Encoding Tokenizer\n\nThis cell initializes a Byte-Pair Encoding (BPE) tokenizer, a method effective for subword tokenization in NLP tasks. The tokenizer is configured with special tokens like `[UNK]`, `[PAD]`, `[CLS]`, `[SEP]`, and `[MASK]`. We use normalization and pre-tokenization strategies suitable for BPE. The tokenizer is trained on a subset of the dataset iteratively and wrapped in `PreTrainedTokenizerFast` for efficient tokenization. Finally, it's applied to both the test and training text data.\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Creating Byte-Pair Encoding tokenizer\nraw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\nraw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\nraw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\ndataset = Dataset.from_pandas(test[['text']])\ndef train_corp_iter(): \n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"text\"]\nraw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\ntokenized_texts_test = []\n\nfor text in tqdm(test['text'].tolist()):\n    tokenized_texts_test.append(tokenizer.tokenize(text))\n\ntokenized_texts_train = []\n\nfor text in tqdm(train['text'].tolist()):\n    tokenized_texts_train.append(tokenizer.tokenize(text))","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-01-15T20:38:37.66305Z","iopub.execute_input":"2024-01-15T20:38:37.663379Z","iopub.status.idle":"2024-01-15T20:40:52.999039Z","shell.execute_reply.started":"2024-01-15T20:38:37.663342Z","shell.execute_reply":"2024-01-15T20:40:52.998051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF-IDF Vectorization with Custom Tokenization\n\nThis cell implements the TF-IDF (Term Frequency-Inverse Document Frequency) vectorization process, customized for specific tokenization needs. The `TfidfVectorizer` is set up with a 3-5 n-gram range and various parameters including sublinear term frequency scaling and unicode accent stripping. Custom functions are used for both tokenization and preprocessing to maintain control over these processes. After fitting the vectorizer to the tokenized test data, we extract the vocabulary. This vocabulary is then used to initialize a new `TfidfVectorizer` which transforms both the training and test datasets. Post-processing, the vectorizer is deleted to free up memory.\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"def dummy(text):\n    return text\n\nvectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, \n    analyzer = 'word',\n    tokenizer = dummy,\n    preprocessor = dummy,\n    token_pattern = None, strip_accents='unicode')\n\nvectorizer.fit(tokenized_texts_test) #train) #test)\n\n# Getting vocab\nvocab = vectorizer.vocabulary_\n\nprint(\"Vocabulary length\", len(vocab))\n\nvectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n                            analyzer = 'word',\n                            tokenizer = dummy,\n                            preprocessor = dummy,\n                            token_pattern = None, strip_accents='unicode'\n                            )\n\ntf_train = vectorizer.fit_transform(tokenized_texts_train)\ntf_test = vectorizer.transform(tokenized_texts_test)\n\ndel vectorizer\ngc.collect()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-01-15T20:40:53.000193Z","iopub.execute_input":"2024-01-15T20:40:53.000464Z","iopub.status.idle":"2024-01-15T20:45:16.906227Z","shell.execute_reply.started":"2024-01-15T20:40:53.000439Z","shell.execute_reply":"2024-01-15T20:45:16.90535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train['label'].values","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-01-15T20:45:16.907556Z","iopub.execute_input":"2024-01-15T20:45:16.907848Z","iopub.status.idle":"2024-01-15T20:45:16.91258Z","shell.execute_reply.started":"2024-01-15T20:45:16.907823Z","shell.execute_reply":"2024-01-15T20:45:16.911621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:45:16.913914Z","iopub.execute_input":"2024-01-15T20:45:16.914719Z","iopub.status.idle":"2024-01-15T20:45:16.92645Z","shell.execute_reply.started":"2024-01-15T20:45:16.914685Z","shell.execute_reply":"2024-01-15T20:45:16.925295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop('label', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:45:16.927631Z","iopub.execute_input":"2024-01-15T20:45:16.928076Z","iopub.status.idle":"2024-01-15T20:45:16.943213Z","shell.execute_reply.started":"2024-01-15T20:45:16.928041Z","shell.execute_reply":"2024-01-15T20:45:16.942438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding a DenseNet Neural Network (block like MLP)\nWith a plot for its output","metadata":{}},{"cell_type":"code","source":"def plot_training_history(history, metrics):\n    loss = history.history['loss']\n    #val_loss = history.history['val_loss']\n\n    epochs = range(len(loss))\n\n    plt.figure(figsize=(12, 6))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label='Training Loss', color=\"blue\")\n    #plt.plot(epochs, val_loss, label='Validation Loss', color=\"red\")\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    plt.title('Metrics')\n    plt.xlabel('Epochs')\n    plt.legend(loc='upper right')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:45:16.944339Z","iopub.execute_input":"2024-01-15T20:45:16.944693Z","iopub.status.idle":"2024-01-15T20:45:16.956522Z","shell.execute_reply.started":"2024-01-15T20:45:16.944654Z","shell.execute_reply":"2024-01-15T20:45:16.955615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USE_DN_ACTIVATION = 'relu'\n\ndef build_DenseNet(X_transformed, num_outputs):\n    \"\"\"\n    Build an multi-block Dense Network with several hidden layers.\n    Args:\n        The full training set, though this is only used for its shape.\n        \n    Returns:\n        The input and output to be used to define a Neural Network Model\n        All hidden layers are prepared between the Input of the training set's width and target output's size (number of labels).\n    \"\"\"    \n    \n    inputs = Input(shape=[X_transformed.shape[1]])# Initial fully connected layer\n    #x = BatchNormalization()(inputs)\n    x = Dense(16)(inputs)\n    x = Activation(USE_DN_ACTIVATION)(x)\n    x = Dropout(.2)(x)\n\n    # Dense blocks\n    num_blocks = 5  \n    \n    for _ in range(num_blocks):\n        # Dense block\n        for _ in range(2):  # Adjust the number of layers in each block as needed\n            y = x\n            #x = BatchNormalization()(x)\n            x = Activation(USE_DN_ACTIVATION)(x)\n            x = Dense(8)(x)#, kernel_initializer='he_normal')(x)\n            x = Dropout(.2)(x)\n            x = Concatenate()([y, x])\n\n    # Final fully connected layers\n    #x = BatchNormalization()(x)\n    x = Activation(USE_DN_ACTIVATION)(x)\n    x = Dense(24)(x) # 64)(x) #, kernel_initializer='he_normal')(x) # was 1800\n    x = Dropout(.2)(x)\n    x = Activation(USE_DN_ACTIVATION)(x)\n\n    # Regression output\n    regression_output = Dense(num_outputs, activation='sigmoid', name='regression_output')(x)#, kernel_initializer='he_normal')(x)\n\n    return inputs, regression_output","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:45:16.957722Z","iopub.execute_input":"2024-01-15T20:45:16.95797Z","iopub.status.idle":"2024-01-15T20:45:16.969186Z","shell.execute_reply.started":"2024-01-15T20:45:16.957949Z","shell.execute_reply":"2024-01-15T20:45:16.968336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom scipy.sparse import csr_matrix\n\ndef csr_to_dataframe(csr_data):\n    # Convert the csr_matrix to a dense format\n    dense_data = csr_data.todense()\n\n    # Create a DataFrame from the dense matrix\n    df = pd.DataFrame(dense_data)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:45:16.970907Z","iopub.execute_input":"2024-01-15T20:45:16.971253Z","iopub.status.idle":"2024-01-15T20:45:16.984222Z","shell.execute_reply.started":"2024-01-15T20:45:16.971222Z","shell.execute_reply":"2024-01-15T20:45:16.983232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def numpy_to_csr(np_pred):\n    # Convert the DataFrame to a sparse matrix\n    sparse_matrix = csr_matrix(np_pred)\n\n    return sparse_matrix","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:45:16.985508Z","iopub.execute_input":"2024-01-15T20:45:16.986528Z","iopub.status.idle":"2024-01-15T20:45:16.994052Z","shell.execute_reply.started":"2024-01-15T20:45:16.986463Z","shell.execute_reply":"2024-01-15T20:45:16.993089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DN_EPOCH_PLAN = 100\n\nearly_stopping = EarlyStopping(\n    patience=6,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nmycount = 0\n\nclass DNKerasClassifierWrapper(BaseEstimator, ClassifierMixin):\n    def __init__(self, keras_model):\n        self.keras_model = keras_model\n\n    def fit(self, scrX,y):\n        #lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: 0.001-(0.001*(step/(DN_EPOCH_PLAN+1))), verbose=0)\n        lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: 0.0001, verbose=0)\n        global mycount\n\n        split_index = int(0.2 * scrX.shape[0])\n\n        X_train_now, X_val, y_train_now, y_val = train_test_split(scrX, \n                                                  y, \n                                                  test_size=0.20,\n                                                  random_state=42+mycount)        \n        \n        X_train_now = self._convert_to_sparse_tensor(X_train_now)\n        X_train_now = tf.sparse.reorder(X_train_now)\n        X_val = self._convert_to_sparse_tensor(X_val)\n        X_val = tf.sparse.reorder(X_val)\n        \n        mycount += 1\n        \n        with strategy.scope():\n            history_1 = self.keras_model.fit(X_train_now, y_train_now, \n                                         epochs=80, \n                                         batch_size=1024,\n                                         callbacks=[lr_callback,early_stopping],\n                                         validation_data = (X_val, y_val),\n                                         verbose=0)\n        plot_training_history(history_1, metrics=[])\n        _ = gc.collect()\n        \n    def predict_proba(self, X_sparse):\n        _ = gc.collect()\n        \n        batch_size = 1000\n        print(\"Predicting for DN\")\n        \n        n_rows = X_sparse.shape[0]\n        prob_class_1 = []\n        initialized = False\n        for start in range(0, n_rows, batch_size):\n            end = min(start + batch_size, n_rows)\n            chunk = self._convert_to_sparse_tensor(X_sparse[start:end])  # Convert to dense format\n            chunk = tf.sparse.reorder(chunk)\n            chunk_predictions = []\n            with strategy.scope():\n                chunk_predictions = self.keras_model.predict(chunk).ravel()\n            if not initialized:\n                initialized = True\n                prob_class_1 = chunk_predictions\n            else:\n                prob_class_1 = np.concatenate([prob_class_1, chunk_predictions])\n            _ = gc.collect()\n            \n        print(\"Prediction completed\")\n\n        prob_class_0 = 1 - prob_class_1\n        _ = gc.collect()\n        return np.vstack((prob_class_0, prob_class_1)).T\n        \n    def _convert_to_sparse_tensor(self, csr):\n        csr_coo = csr.tocoo()\n        indices = np.mat([csr_coo.row, csr_coo.col]).transpose()\n        return tf.SparseTensor(indices, csr_coo.data, csr_coo.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:45:16.995392Z","iopub.execute_input":"2024-01-15T20:45:16.995914Z","iopub.status.idle":"2024-01-15T20:45:17.012293Z","shell.execute_reply.started":"2024-01-15T20:45:16.995881Z","shell.execute_reply":"2024-01-15T20:45:17.011439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many DenseNet models to build - Do not change without adding to weights.\n\nTOTAL_MODELS = 1","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:45:17.013747Z","iopub.execute_input":"2024-01-15T20:45:17.014017Z","iopub.status.idle":"2024-01-15T20:45:17.027995Z","shell.execute_reply.started":"2024-01-15T20:45:17.013988Z","shell.execute_reply":"2024-01-15T20:45:17.027111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Learning with Multiple Classifiers\n\nThis cell sets up an ensemble learning model using various classifiers, each with its specific configurations:\n\n1. **Multinomial Naive Bayes**: \n   A `MultinomialNB` classifier with a smoothing parameter `alpha` set to 0.02.\n\n2. **SGD Classifier**: \n   An `SGDClassifier` for linear models with a modified Huber loss function, a maximum of 8000 iterations, and a tolerance of 1e-4 for stopping criteria.\n\n3. **LightGBM Classifier**: \n   An `LGBMClassifier` configured with custom parameters such as learning rate, lambda values, max depth, and more, specified in the `p6` dictionary.\n\n4. **CatBoost Classifier**: \n   A `CatBoostClassifier` with 1000 iterations, silent mode (no verbose output), specific learning rate, L2 regularization, and a subsampling rate of 0.4.\n\n5. **Ensemble Model - Voting Classifier**: \n   A `VotingClassifier` that combines the above models (`MultinomialNB`, `SGDClassifier`, `LGBMClassifier`, `CatBoostClassifier`) using soft voting. The weights for each classifier in the ensemble are specified, with a focus on the three non-Naive Bayes models.\n   \n6. **Added a DenseNet (blocks an FNN)**:\n   This addition was made by Thomas Gamet as are weights adjustments.\n\nThe ensemble model is then trained on the transformed training data (`tf_train`) and labels (`y_train`). Finally, the ensemble model is used to predict probabilities on the test dataset (`tf_test`), and garbage collection is run to manage memory.\n``\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, precision_recall_curve\nfrom scipy.optimize import minimize\nfrom sklearn.metrics import log_loss\nimport time\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n#def minobjective(weights, truthset, pred1, pred2, pred3, pred4, pred5):\n#    weighted_sum = weights[0] * pred1 + weights[1] * pred2 + weights[2] * pred3 + weights[3] * pred4 + weights[4] * pred5\n#    objectiveval = roc_auc_score(truthset, weighted_sum)\n#    return -objectiveval\n\ndef calculate_voting_bpe(tf_train, tf_test, y_train):\n    neg_deltas = [0.0, 0.0, 0.0, 0.0, 0.0]\n    \n    clf = MultinomialNB(alpha=0.02)\n    sgd_model = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\") \n    p6={'n_iter': 3000,\n        'verbose': -1,'objective': 'cross_entropy','metric': 'auc',\n        'learning_rate': 0.00582, 'colsample_bytree': 0.78,\n        'colsample_bynode': 0.8, 'lambda_l1': 4.56296,\n        \"device\": \"gpu\",\n        'gpu_device_id': 0,\n        'lambda_l2': 2.97485, 'min_data_in_leaf': 115, 'max_depth': 23}\n    lgb=LGBMClassifier(**p6)\n    print(\"tf_train.shape=\", tf_train.shape)\n    # Fit classifiers and make predictions\n    print(\"clf.fit() in progress\")\n    clf.fit(tf_train, y_train)\n    predictions_mnb = clf.predict_proba(tf_test)[:, 1]\n    tpredictions_mnb = clf.predict_proba(tf_train)[:, 1]\n    neg_deltas[0] = 1 - roc_auc_score(y_train, tpredictions_mnb)\n    del tpredictions_mnb\n    del clf\n    _ = gc.collect()\n\n    print(\"sgd_model.fit() in progress\")\n    sgd_model.fit(tf_train, y_train)\n    predictions_sgd = sgd_model.predict_proba(tf_test)[:, 1]\n    tpredictions_sgd = sgd_model.predict_proba(tf_train)[:, 1]\n    neg_deltas[1] = 1 - roc_auc_score(y_train, tpredictions_sgd)\n    del tpredictions_sgd\n    del sgd_model\n    _ = gc.collect()\n    \n    print(\"lgb.fit() in progress\")\n    lgb.fit(tf_train, y_train)\n    predictions_lgb = lgb.predict_proba(tf_test)[:, 1]\n    tpredictions_lgb = lgb.predict_proba(tf_train)[:, 1]\n    neg_deltas[2] = 1 - roc_auc_score(y_train, tpredictions_lgb)\n    print('done with lightgbm')\n    del tpredictions_lgb\n    del lgb\n    _ = gc.collect()\n    \n    #TOTAL_MODELS = 1\n\n    dn_model_wrapper_list = []\n\n    for model in range(TOTAL_MODELS):\n        inputs, outputs = build_DenseNet(tf_train, 1)\n        dnmodelx = Model(inputs=inputs, outputs=outputs)\n        dnmodelx.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n                        loss='binary_crossentropy', metrics=[])\n        dnmodelx.summary()\n        dn_keras_wrapper = DNKerasClassifierWrapper(dnmodelx)\n        dn_model_wrapper_list.append(dn_keras_wrapper)\n    \n    print(\"dn_model_wrapper_list=\",dn_model_wrapper_list)\n\n    print(\"dn0.fit() in progress\")\n    dn_model_wrapper_list[0].fit(tf_train, y_train)\n    predictions_dn0 = dn_model_wrapper_list[0].predict_proba(tf_test)[:, 1]\n    tpredictions_dn0 = dn_model_wrapper_list[0].predict_proba(tf_train)[:, 1]\n    neg_deltas[3] = 1 - roc_auc_score(y_train, tpredictions_dn0)\n    del dn_model_wrapper_list # add to the last use of dn#\n    del tpredictions_dn0\n    _ = gc.collect()\n    print('done with dn0')\n    \n    cat=CatBoostClassifier(iterations=3000, #task_type='GPU', bootstrap_type='Bernoulli', \n                       verbose=0,\n                       l2_leaf_reg=6.65913,\n                       learning_rate=0.005599,\n                       subsample = 0.35,\n                       allow_const_label=True,loss_function = 'CrossEntropy')\n    \n    try:\n        cat.fit(tf_train, y_train)\n        predictions_cat = cat.predict_proba(tf_test)[:, 1]\n        tpredictions_cat = cat.predict_proba(tf_train)[:, 1]\n        neg_deltas[4] = 1 - roc_auc_score(y_train, tpredictions_cat)\n        print('done with catboost')\n        del cat\n        #del tpredictions_cat\n        _ = gc.collect()\n    except Exception:\n        print(\"Skipping catboost in the dev run as it cannot train on test only vocab.\")\n        # the following line prevents failure of the test run to calculate the weights below\n        predictions_cat = predictions_dn0\n        #tpredictions_cat = tpredictions_dn0\n        \n    del tpredictions_cat\n    _ = gc.collect()\n    \n    #from sklearn.svm import SVC\n    #\n    #the_model = SVC(kernel='linear',\n    #                    probability=True, \n    #                    #max_iter=8000,\n    #                    verbose=0)\n    #print(\"+++++++++++++ svm_model.fit() in progress\")\n    #the_model.fit(tf_train, y_train)\n    #predictions_svc = the_model.predict_proba(tf_test)[:, 1]\n    #print(\"+++++++++++++ wrappinng up the predictions\")\n    \n    # Initial weights (equal weights for pred1, pred2, pred3)\n    #initial_weights = [1/5, 1/5, 1/5, 1/5, 1/5]\n\n    # Constraints: weights should sum to 1\n    # constraints = [{'type': 'eq', 'fun': lambda w: sum(w) - 1}]\n    # Nedler-Mead does not use the constraints (kept in case a differentiable option is tried\n\n    # Bounds: each weight should be between 0 and 1\n    #bounds = [(0, 1)] * 5\n\n    #print(\"Attempting to find an optimal set of weights\")\n    #starttime = time.time()\n    #options = {'maxiter': 1000, 'maxfev': 5000}\n    #result = minimize(minobjective, initial_weights, args=(y_train, \n    #                                                    tpredictions_mnb,\n    #                                                    tpredictions_sgd,\n    #                                                    tpredictions_lgb,\n    #                                                    tpredictions_dn0,\n    #                                                    tpredictions_cat),\n    #                  method='Nelder-Mead', bounds=bounds, options=options) # constraints=constraints)\n\n    # Get the optimized weights\n    #weights = result.x\n    #print(\"weights =\",weights)\n    #print(\"Took\",time.time()-starttime,\"seconds.\")\n    \n    # Now assigned dynamic weights LB score dropped from .96 to .946\n    #min_delta = min(neg_deltas)\n    #print(\"neg_deltas=\",neg_deltas)\n    #weights = [1.0, 1.0, 1.0, 1.0, 1.0]\n    #for i in range(len(weights)):\n    #    weights[i] = min_delta/neg_deltas[i]\n    #print (\"Dynamically adjusted weights =\", weights)\n    \n    # Define weights\n    #weights = [0.07,0.24,0.23,0.23,0.23] # 0.893 with DN using 80% training data\n    #weights = [0.07,0.31,0.62] # LB 0.956 with 2000 and 4000 trees\n    #weights = [0.06,0.47,0.47] # LB 0.956 with 4000 trees\n    #weights = [0.00,0.00,0.00,1.00]\n    weights = [0.02,0.20,0.10,0.24,0.44]\n\n    # Calculate weighted average of predictions\n    final_preds = (weights[0] * predictions_mnb + weights[1] * predictions_sgd + weights[2] * predictions_lgb +\n                   weights[3] * predictions_dn0 + weights[4] * predictions_cat) / sum(weights)\n    #final_preds = (weights[0] * tpredictions_mnb + weights[1] * tpredictions_sgd + weights[2] * tpredictions_lgb +\n    #               weights[3] * tpredictions_dn0 + weights[4] * tpredictions_cat) / sum(weights)\n    #final_preds = predictions_dn0\n    \n    del predictions_mnb\n    del predictions_sgd\n    del predictions_lgb\n    del predictions_dn0\n    del predictions_cat\n    #del predictions_svc\n    \n    #del tpredictions_mnb\n    #del tpredictions_sgd\n    #del tpredictions_lgb\n    #del tpredictions_dn0\n    #del tpredictions_cat\n    \n    # Garbage collection\n    gc.collect()\n\n    return final_preds\n\nfinal_preds_bpe = calculate_voting_bpe(tf_train, tf_test, y_train)\n_ = gc.collect()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-15T20:45:17.02956Z","iopub.execute_input":"2024-01-15T20:45:17.029848Z","iopub.status.idle":"2024-01-15T20:48:25.73542Z","shell.execute_reply.started":"2024-01-15T20:45:17.029824Z","shell.execute_reply":"2024-01-15T20:48:25.734344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from sklearn.metrics import roc_auc_score \n\nprint(y_train.shape) \nprint(final_preds_bpe.shape)\n\ntry: \n    roc_auc = roc_auc_score(y_train, final_preds_bpe) \n    print(\"ROC AUC Score (if with y_test then this is the baseline):\", roc_auc) \nexcept Exception: \n    print(\"Trying to igore a mismatch of test and training data on ROC AUC calculation (this can only use all training data)\")","metadata":{"execution":{"iopub.status.busy":"2024-01-08T00:03:19.310247Z","iopub.execute_input":"2024-01-08T00:03:19.311112Z","iopub.status.idle":"2024-01-08T00:03:19.332651Z","shell.execute_reply.started":"2024-01-08T00:03:19.311077Z","shell.execute_reply":"2024-01-08T00:03:19.331768Z"}}},{"cell_type":"markdown","source":"del tokenized_texts_test, tokenized_texts_train, dataset, raw_tokenizer, tokenizer\ndel trainer\ndel tf_train, tf_test, y_train\ndel vocab\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-05T19:13:45.6409Z","iopub.execute_input":"2024-01-05T19:13:45.641312Z","iopub.status.idle":"2024-01-05T19:13:47.870341Z","shell.execute_reply.started":"2024-01-05T19:13:45.641276Z","shell.execute_reply":"2024-01-05T19:13:47.869184Z"}}},{"cell_type":"markdown","source":"# The whole spe thing looks like little more than a second build of the models\n## Let's comment out all of \"SPE\" and see how well just the \"BPE\" models do.","metadata":{}},{"cell_type":"markdown","source":"# Go as far as possible, make sure any references to the training and test data\n# are truly returned to the heap and hope it is not fragmented beyond use.\n\ndel test\ndel sub\ndel train\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-05T19:13:47.883798Z","iopub.execute_input":"2024-01-05T19:13:47.884209Z","iopub.status.idle":"2024-01-05T19:13:48.280256Z","shell.execute_reply.started":"2024-01-05T19:13:47.884176Z","shell.execute_reply":"2024-01-05T19:13:48.27898Z"}}},{"cell_type":"markdown","source":"# Before we can get started we need to reload the training data\nIt was deleted and garbage collected so that that we really start off clean\nto avoid out of memory errors.","metadata":{}},{"cell_type":"markdown","source":"test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\nsub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\ntrain = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')","metadata":{"execution":{"iopub.status.busy":"2024-01-05T19:13:48.317658Z","iopub.execute_input":"2024-01-05T19:13:48.317995Z","iopub.status.idle":"2024-01-05T19:13:49.387481Z","shell.execute_reply.started":"2024-01-05T19:13:48.317967Z","shell.execute_reply":"2024-01-05T19:13:49.386627Z"}}},{"cell_type":"markdown","source":"train = train.drop_duplicates(subset=['text'])\ntrain.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T19:13:49.388744Z","iopub.execute_input":"2024-01-05T19:13:49.389127Z","iopub.status.idle":"2024-01-05T19:13:49.466358Z","shell.execute_reply.started":"2024-01-05T19:13:49.389093Z","shell.execute_reply":"2024-01-05T19:13:49.465378Z"},"_kg_hide-output":true}},{"cell_type":"markdown","source":"We can now get serious about the second way to build encodings for training.","metadata":{}},{"cell_type":"markdown","source":"VOCAB_SIZE = 42000\n# Creating Byte-Pair Encoding tokenizer\nprint(\"Working on the the second BPE (it's not really SPE)\")\nraw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\nraw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\nraw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\ndataset = Dataset.from_pandas(test[['text']])\ndef train_corp_iter(): \n    for i in range(0, len(dataset), 500):\n        yield dataset[i : i + 500][\"text\"]\nraw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\ntokenized_texts_test = []\n\nfor text in tqdm(test['text'].tolist()):\n    tokenized_texts_test.append(tokenizer.tokenize(text))\n\ntokenized_texts_train = []\n\nfor text in tqdm(train['text'].tolist()):\n    tokenized_texts_train.append(tokenizer.tokenize(text))\n    \n########################################\n    \nvectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',\n    tokenizer = dummy,\n    preprocessor = dummy,\n    token_pattern = None, strip_accents='unicode')\n\nvectorizer.fit(tokenized_texts_test) #train)\n\n# Getting vocab\nvocab = vectorizer.vocabulary_\n\nprint(\"Vocabulary length\", len(vocab))\n\nvectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n                            analyzer = 'word',\n                            tokenizer = dummy,\n                            preprocessor = dummy,\n                            token_pattern = None, strip_accents='unicode'\n                            )\n\nprint(\"vectorizer fit_transform in progress\")\ntf_train = vectorizer.fit_transform(tokenized_texts_train)\nprint(\"vectorizer transform in progress\")\ntf_test = vectorizer.transform(tokenized_texts_test)\n\ndel vectorizer\ngc.collect()\n\ny_train = train['label'].values\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-01-05T19:13:49.467888Z","iopub.execute_input":"2024-01-05T19:13:49.468236Z","iopub.status.idle":"2024-01-05T19:20:51.070583Z","shell.execute_reply.started":"2024-01-05T19:13:49.468204Z","shell.execute_reply":"2024-01-05T19:20:51.06946Z"}}},{"cell_type":"markdown","source":"def calculate_voting(tf_train, tf_test, y_train):\n    # Initialize classifiers\n    clf = MultinomialNB(alpha = 0.02)\n    sgd_model = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\") \n    p7={'n_iter': 2000,\n        'verbose': -1,'objective': 'cross_entropy','metric': 'auc',\n        'learning_rate': 0.0058, 'colsample_bytree': 0.78,\n        'colsample_bynode': 0.8, 'lambda_l1': 4.563,\n        \"device\": \"gpu\",\n        'gpu_device_id': 0,\n        'lambda_l2': 2.97, 'min_data_in_leaf': 112, 'max_depth': 21}\n    lgb = LGBMClassifier(**p7)\n    cat=CatBoostClassifier(iterations=2000,\n                       verbose=0,\n                       l2_leaf_reg=6.659,\n                       learning_rate=0.0056,\n                       subsample = 0.4,\n                       allow_const_label=True,loss_function = 'CrossEntropy')\n    \n    print(\"tf_trsin.shape=\", tf_train.shape)\n    # Fit classifiers and make predictions\n    print(\"clf.fit() in progress\")\n    clf.fit(tf_train, y_train)\n    predictions_mnb = clf.predict_proba(tf_test)[:, 1]\n    del clf\n    _ = gc.collect()\n\n    print(\"sgd_model.fit() in progress\")\n    sgd_model.fit(tf_train, y_train)\n    predictions_sgd = sgd_model.predict_proba(tf_test)[:, 1]\n    del sgd_model\n    _ = gc.collect()\n    \n    print(\"lgb.fit() in progress\")\n    lgb.fit(tf_train, y_train)\n    predictions_lgb = lgb.predict_proba(tf_test)[:, 1]\n    print('done with lightgbm')\n    del lgb\n    _ = gc.collect()\n    \n    dn_model_wrapper_list = []\n\n    for model in range(TOTAL_MODELS):\n        inputs, outputs = build_DenseNet(tf_train, 1)\n        dnmodelx = Model(inputs=inputs, outputs=outputs)\n        dnmodelx.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n                          loss='binary_crossentropy', metrics=[])\n        dnmodelx.summary()\n        dn_keras_wrapper = DNKerasClassifierWrapper(dnmodelx)\n        dn_model_wrapper_list.append(dn_keras_wrapper)\n    \n    print(\"dn_model_wrapper_list=\",dn_model_wrapper_list)\n    \n    print(\"dn0.fit() in progress\")\n    dn_model_wrapper_list[0].fit(tf_train, y_train)\n    predictions_dn0 = dn_model_wrapper_list[0].predict_proba(tf_test)[:, 1]\n    del dn_model_wrapper_list\n    _ = gc.collect()\n    print('done with dn0')\n    \n    predictions_cat = []\n    try:\n        cat.fit(tf_train, y_train)\n        predictions_cat = cat.predict_proba(tf_test)[:, 1]\n        print('done with catboost')\n        del cat\n    except Exception:\n        print(\"Skipping catboost in the dev run as it cannot train on test only vocab.\")\n        predictions_cat = predictions_dn0\n    \n    # Define weights\n    weights = [0.07,0.24,0.23,0.23,0.23]\n    #weights = [0.07,0.31,0.31,0.31]\n    #weights = [0.07,0.31,0.62] # LB 0.956 with 2000 and 4000 trees\n    #weights = [0.06,0.47,0.47] # LB 0.956 with 4000 trees\n    #weights = [0.00,0.00,0.00,1.00]\n\n    # Calculate weighted average of predictions\n    #final_preds = (weights[0] * predictions_mnb + weights[1] * predictions_sgd + weights[2] * predictions_lgb + weights[3] * predictions_cat) / sum(weights)\n    final_preds = (weights[0] * predictions_mnb + \n                   weights[1] * predictions_sgd + \n                   weights[2] * predictions_lgb + \n                   weights[3] * predictions_dn0 + \n                   weights[4] * predictions_cat) / sum(weights)\n    \n    del predictions_mnb\n    del predictions_sgd\n    del predictions_lgb\n    del predictions_dn0\n    del predictions_cat\n\n    # Garbage collection\n    gc.collect()\n\n    return final_preds\n\nfinal_preds_spe = calculate_voting(tf_train, tf_test, y_train)\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-05T19:20:51.071894Z","iopub.execute_input":"2024-01-05T19:20:51.07223Z","iopub.status.idle":"2024-01-05T19:21:42.540763Z","shell.execute_reply.started":"2024-01-05T19:20:51.072202Z","shell.execute_reply":"2024-01-05T19:21:42.539469Z"}}},{"cell_type":"markdown","source":"# Final Submission and Closing Remarks\n\n## Submission Preparation\nIn this final cell, we prepare our submission:\n\n1. **Ensemble Prediction Averaging**:\n   We combine the predictions from both the Byte-Pair Encoding (BPE) and Sentencepiece Encoding (SPE) models by averaging them. This approach helps in harnessing the strengths of both models.\n\n   ```python\n   sub['generated'] ~= (final_preds_bpe + final_preds_spe) / 2\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"#sub['generated'] = 0.51 * np.array(final_preds_bpe) + 0.49 * final_preds_spe\nsub['generated'] = np.array(final_preds_bpe)\nsub.to_csv('submission.csv', index=False)\nsub","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-01-15T20:48:25.736943Z","iopub.execute_input":"2024-01-15T20:48:25.737377Z","iopub.status.idle":"2024-01-15T20:48:25.756092Z","shell.execute_reply.started":"2024-01-15T20:48:25.737338Z","shell.execute_reply":"2024-01-15T20:48:25.75519Z"},"trusted":true},"execution_count":null,"outputs":[]}]}